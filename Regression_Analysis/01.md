# Introduction


- Regression analysis is regarded as one of the oldest and most time-tested topics in mathematical statistics. The earliest form of the linear regression was the least squares method(最小二乘法).

- Nowadays, linear regression plays an important role, e.g., in machine learning. The linear regression algorithm is one of the fundamental supervised machine-learning algorithms due to its relative simplicity and completeness of theory.

- Multiple Linear Regression can be used as an activation function in a layer of (Deep) Artificial Neural Networks.


### Simple Linear Regression
- The theory and practice of regression deals with the following situation. There are n pairs of values(real numbers)

$$D_{tr}=[(x_1,y_1), ..., (x_n, y_n)]$$

$y_i$ : values of a dependent variable y a.k.a called the "outcome" or "reponse" variable, or a 'label' in machine learning jargon（行话）

$x_i$ : values of an independent variable x a.k.a 'predictor' 'covariate', 'explanatory variable' or feature. 

$$y = \beta_0 + \beta_1 x$$

is called the theoretic line of regression. This line is not likely to hold exactly in D_{tr}. The idea is to fit a line of this kiind to the data in $D_{tr}$ in an approximate sense.

- We fit a line like the theoretic line of regression by estimating the parameters $\beta_0$ and $beta_1$ by the method of least squares. 

- I.e., we minimize the sum of the squared **vertical distances** (the distance between two vertical positions.) between the theoretic line and values of the response y, i.e., we minimize

$$Q(\beta_0, \beta_1)=\sum_{i=1}^{n}(y_i -\beta_0 - \beta_1 x_i)^2$$


##### Vertical Distances

![](Pictures/0101.png)

$\beta_0$ in is called the intercept, $\beta_1$ is the slope. The values $\hat{\beta_0}$ and $\hat{\beta_1}$ attaining the minimum are known as the Least Squares-Estimates(LSE) of $\beta_0$ and $\beta_1$ ,  respectively.

![](Pictures/0102.png)

The straight line

$$\hat{y} = \hat{\beta_0} + \hat{\beta_1} x$$

is called the **estimated line of regression** or the predictor. The vertical distances $e_i$ from $y_i$ to the estimated line of regression at $x_i$

$\hat{e_i} = y_i - \hat{y_i} =  y_i - \hat{\beta_0} - \hat{\beta_1} x_i$ 

are called observed least squares residuals. $Q_0$ is defined as

$$Q_0:=Q(\hat{\beta_0}, \hat{\beta_1}) = \sum_{i=1}^{n}\hat{e_i}^2$$